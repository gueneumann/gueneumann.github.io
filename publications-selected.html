<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
	<title>Technical publications written by G&uuml;nter Neumann</title>
	<link rel="stylesheet" media="screen,projection"  title="GN home page style" href="styles.css" type="text/css" />
	
	<script type="text/javascript" src="script.js"></script>
	
</head>

<body>
	<div id="wrapper">
		
		<div id="sitename">
			<h1>g&uuml;nter neumann</h1>
		</div>
		
		<div id="header">
			<!-- Header so ändern, dass links Bild mit meinem Namen und rechts DFKI und LT-lab erscheinen-->
			<table>
				<colgroup>
				<col width="30%" />
				<col width="60%" />
				<col width="10%" />
				</colgroup>
				<tbody>
					<tr>
						<td> <img src="images/gn-liest.jpg" width="115" height="168" alt="Image of G&uuml;nter Neumann"/></td>
						<td><h2>German Research Center for Artificial Intelligence</h2></td>
						<td><img src="images/logo2.jpg" alt="Image of DFKI logo"/></td>
					</tr>
				</tbody>
			</table>
		</div>
		
		<div id="body" class="clear">
			
			<div id="sidebar" class="column-left">
				<!-- Sidebar nun so ändern, dass hier die verschiedenen links aufgelistet werden und auf die entsprechenden Websites zeigen,
				wobei ich bei jedem item den body text ändern muss und auch die link adressen-->
				<ul>	
					<li>
						<h4>Navigate</h4>
						<ul>
							<li><a href="index.html" title="guenter neumann home page">Home</a></li>
							<li><a href="contact.html" title="guenter neumann contact information">Contact</a></li>
							<li><a href="publications-selected.html" title="guenter neumann publication page">Publications</a></li>
							<li><a href="courses.html" title="courses web pages">Courses</a></li>
							<li><a href="GN_CV.pdf" title="Curriculum Vitae">Curriculum Vitae</a></li>
							<li><a href="talks.html" title="presentations">Selected Presentations</a></li>
							<li><a href="projects.html" title="projects">Projects</a></li>
							<li><a href="resources.html" title="resources">Software</a></li>
							<li><a href="memberships.html" title="scientific memberships">Memberships</a></li>
							<li><a href="partner.html" title="cooperation">Partners</a></li>
							<li><a href="oldnews.html" title="oldnews">Old News</a></li>
						</ul>
					</li>
				</ul>
				<h4>Hello!</h4>
				<p>This is a selection of my publications. The full list can be found <a href="publications.html">here</a>!</p>
				<p>Most papers are available online: just click a paper's title to read the abstract and to get a download link to the full paper's
					content. Occasionally, I also add further information, e.g., about available software.</p>
					
				<p>You may also check my papers at 
					<a href="http://dblp.uni-trier.de/pers/hd/n/Neumann:G=uuml=nter">DBLP</a>,
					<a href="https://aclanthology.org/people/g/gunter-neumann/">ACL Anthology</a>,
					<a href="https://scholar.google.com/citations?hl=en&amp;user=42gs8NAAAAAJ" title="Google scholar">
						Google Scholar</a>,
					<a href="https://www.semanticscholar.org/author/G.-Neumann/143792905" 
						title="G. Neumann | Semantic Scholar">
						Semantic Scholar</a>,
					<a href="https://www.dfki.de/en/web/research/research-departments/multilinguality-and-language-technology/publications-mlt?tx_dfkimanager_publicationoverview%5Baction%5D=list&tx_dfkimanager_publicationoverview%5Bcontroller%5D=Publication&tx_dfkimanager_publicationoverview%5BpersonId%5D=3284&cHash=56618bd2388fdacd1e25d5e9f152827f" title="DFKI MLT - Publications">DFKI publication</a>.
				</p>
				</div>
			
<div id="content" class="column-right">
<h3>Publications (full list <a href="publications.html">here</a>)</h3>
				
<div id="publications">

	<ul>
		<li>
				<span>Julian Schlenker, Jenny Kunz, Tatiana Anikina, G&uuml;nter Neumann, and Simon Ostermann 
					(<strong>2025</strong>)</span>
					<a onclick="toggleItem('ref163Abstract')">Only for the Unseen Languages, Say the Llamas: 
						On the Efficacy of Language Adapters for Cross-lingual Transfer in English-centric LLMs</a>, 
						 Proceedings of the 63st Annual Meeting of the Association for Computational Linguistics 
						 (Student Research Workshop). ACL Student Research Workshop (ACL-IJCNLP-SRW-2025), located at ACL
						(<a href="https://aacl2025-srw.github.io/" title="ACL-IJCNLP-SRW-2025">ACL-IJCNLP-SRW-2025</a>), 
						Italy, 2025.

					<div class="abstract" id="ref163Abstract" style="display: none;">
						<p>
							Most state-of-the-art large language models (LLMs) are trained mainly on English data, 
							limiting their effectiveness on non-English, especially low-resource, languages. 
							This study investigates whether language adapters can facilitate cross-lingual transfer 
							in English-centric LLMs. We train language adapters for 13 languages using Llama 2 (7B) 
							and Llama 3.1 (8B) as base models, and evaluate their effectiveness on two downstream tasks 
							(MLQA and SIB-200) using either task adapters or in-context learning. Our results 
							reveal that language adapters improve performance for languages not seen during pre-training, 
							but provide negligible benefit for seen languages. These findings highlight the limitations 
							of language adapters as a general solution for multilingual adaptation in English-centric LLMs.	 
						</p>

					<div style="text-align:left"> <a class="small" href="https://www.dfki.de/fileadmin/user_upload/import/15980_ACL_SRW_2025.pdf">
						Download</a></div>
			</li>
		<li>
			<span>Muhammad Umer Tariq Butt; Stalin Varanasi, and G&uuml;nter Neumann (<strong>2025</strong>)</span>
				<a onclick="toggleItem('ref161Abstract')">Enabling Low-Resource Language Retrieval: 
					Establishing Baselines for Urdu MS MARCO</a>, Proceedings of 47th 
					European Conference on Information Retrieval
					(<a href="https://ecir2025.eu/" title="TextGraphs 2024">ECIR 2025</a>), 
					Italy, 2025.

				<div class="abstract" id="ref161Abstract" style="display: none;">
					<p>
						As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, 
						addressing the needs of low-resource languages remains a significant challenge. 
						This paper introduces the first large-scale Urdu IR dataset, created by translating the MS MARCO dataset 
						through machine translation.We establish baseline results through zero-shot learning for 
						IR in Urdu and subsequently apply the mMARCO multilingual IR methodology to this newly translated dataset. 
						Our findings demonstrate that the fine-tuned model (Urdu-mT5-mMARCO) achieves a 
						Mean Reciprocal Rank (MRR@10) of 0.247 and a Recall@10 of 0.439, representing significant 
						improvements over zero-shot results and showing the potential for expanding IR access for Urdu speakers. 
						By bridging access gaps for speakers of low-resource languages, this work not only advances 
						multilingual IR research but also emphasizes the ethical and societal importance of 
						inclusive IR technologies. This work provides valuable insights into the 
						challenges and solutions for improving language representation and lays the groundwork for 
						future research, especially in South Asian languages, which can benefit 
						from the adaptable methods used in this study.	 
					</p>

				<div style="text-align:left"> <a class="small" href="https://arxiv.org/abs/2412.12997">
					Download</a></div>
		</li>
		<li>
			<span>Noon Pokaratsiri, Saadullah Amin, and G&uuml;nter Neumann (<strong>2024</strong>)</span>
				<a onclick="toggleItem('ref160Abstract')">Towards Understanding Attention-based Reasoning through Graph
					Structures in Medical Codes Classification</a>, Proceedings of TextGraphs-17: 
					Graph-based Methods for Natural Language Processing at ACL-2024 
					(<a href="https://aclanthology.org/volumes/2024.textgraphs-1/" title="TextGraphs 2024">TextGraphs 2024</a>), 
					Bangkok, Thailand, 2024.

				<div class="abstract" id="ref160Abstract" style="display: none;">
					<p>
						A common approach to automatically assigning diagnostic and procedural clinical codes to health records 
						is to solve the task as a multi-label classification problem. Difficulties associated with this task stem 
						from domain knowledge requirements, long document texts, large and imbalanced label space, reflecting the 
						breadth and dependencies between medical diagnoses and procedures. Decisions in the healthcare domain also 
						need to demonstrate sound reasoning, both when they are correct and when they are erroneous. Existing works 
						address some of these challenges by incorporating external knowledge, which can be encoded into a graph-structured format. 
						Incorporating graph structures on the output label space or between the input document and output 
						label spaces have shown promising results in medical codes classification. Limited focus has been put on 
						utilizing graph-based representation on the input document space. To partially bridge this gap, 
						we represent clinical texts as graph-structured data through the UMLS Metathesaurus; we explore implicit 
						graph representation through pre-trained knowledge graph embeddings and explicit domain-knowledge 
						guided encoding of document concepts and relational information through graph neural networks. 
						Our findings highlight the benefits of pre-trained knowledge graph embeddings in understanding 
						model's attention-based reasoning. In contrast, transparent domain knowledge guidance in graph 
						encoder approaches is overshadowed by performance loss. 
						Our qualitative analysis identifies limitations that contribute to prediction errors.	 
					</p>

				<div style="text-align:left"> <a class="small" href="https://aclanthology.org/2024.textgraphs-1.6.pdf">
					Download</a></div>
		</li>
		<li>
			<span>Tanja B&auml;umel, Soniya Vijayakumar, Josef van Genabith, G&uuml;nter Neumann, and Simon Ostermann (<strong>2023</strong>)</span>
				<a onclick="toggleItem('ref159Abstract')">Investigating the Encoding of Words in BERT's Neurons Using Feature Textualization</a>, 
				Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. 
					(<a href="https://aclanthology.org/2023.blackboxnlp-1.20/" title="BlackboxNLP 2023">BlackboxNLP-2023</a>), 
					Singapore, 2023.

				<div class="abstract" id="ref159Abstract" style="display: none;">
					<p>
						Pretrained language models (PLMs) form the basis of most state-of-the-art NLP technologies. 
						Nevertheless, they are essentially black boxes: Humans do not have a clear understanding of 
						what knowledge is encoded in different parts of the models, especially in individual neurons. 
						A contrast is in computer vision, where feature visualization provides a decompositional interpretability 
						technique for neurons of vision models. Activation maximization is used to synthesize inherently 
						interpretable visual representations of the information encoded in individual neurons. 
						Our work is inspired by this but presents a cautionary tale on the interpretability of single neurons, 
						based on the first large-scale attempt to adapt activation maximization to NLP, and, more specifically, 
						large PLMs. We propose feature textualization, a technique to produce dense representations of neurons 
						in the PLM word embedding space. We apply feature textualization to the BERT model to investigate 
						whether the knowledge encoded in individual neurons can be interpreted and symbolized. 
						We find that the produced representations can provide insights about the knowledge encoded in 
						individual neurons, but that individual neurons do not represent clear-cut symbolic 
						units of language such as words. Additionally, we use feature textualization 
						to investigate how many neurons are needed to encode words in BERT.	 
					</p>

				<div style="text-align:left"> <a class="small" href="https://aclanthology.org/2023.blackboxnlp-1.20.pdf">
					Download</a></div>
		</li>
		<li>
			<span>Stalin Varanasi, Muhammad Umer Butt, and G&uuml;nter Neumann (<strong>2023</strong>)</span>
			<a onclick="toggleItem('ref158Abstract')">AutoQIR: Auto-Encoding Questions with Retrieval Augmented Decoding
				for Unsupervised Passage Retrieval and Zero-shot Question Generation</a>, 
				Proceedings of Recent Advances in Natural Language Processing (<a
				href="https://ranlp.org/ranlp2023/index.php/accepted-papers/" title="RANLP 2023">RANLP-2023</a>), Bulgaria, 2023.

		<div class="abstract" id="ref158Abstract" style="display: none;">
			<p>
				Dense passage retrieval models have become state-of-the-art for information retrieval on many Open-domain Question Answering
				(ODQA) datasets. However, most of these models rely on supervision obtained from the ODQA datasets, which hinders their
				performance
				in a low-resource setting. 
				Recently, retrieval-augmented language models have been proposed to improve both zero-shot
				and supervised information retrieval. However, these models have pre-training tasks that are agnostic to the target task of
				passage retrieval.
				In this work, we propose Retrieval Augmented Auto-encoding of Questions for zeroshot
				dense information retrieval. Unlike other pre-training methods, our pre-training method
				is built for target information retrieval, thereby making the pre-training more efficient. Our
				method consists of a dense IR model for encoding questions and retrieving documents during
				training and a conditional language model that maximizes the question’s likelihood by
				marginalizing over retrieved documents. As a by-product, we can use this conditional language
				model for zero-shot question generation from documents. We show that the IR model 
				obtained through our method improves the current state-of-the-art of zero-shot dense information
				retrieval, and we improve the results even further by training on a synthetic corpus
				created by zero-shot question generation.	 
			</p>

		<div style="text-align:left"> <a class="small" href="publications/new-ps/AutoQIR_RANLP_23.pdf">
			Download</a></div>
		</li>
		
		<li>
			<span>Saadullah Amin, Pasquale Minervini, David Chang, Pontus Stenetorp, and G&uuml;nter Neumann (<strong>2022</strong>)</span>
			<a onclick="toggleItem('ref157Abstract')">MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation
				Extraction</a>, Proceedings of The 29th International Conference on Computational Linguistics 
				(<a href="https://coling2022.org/" title="COLING 2022">Coling-2022</a>), 
				October 12-17, 2022, Gyeongju, Republic of Korea

		<div class="abstract" id="ref157Abstract" style="display: none;">
			<p>
				Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing
				domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing
				knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering
				a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation
				extraction benchmarks and found a significant overlap between training and test relationships ranging from 26% to 86%.
				Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no
				train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark
				MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is
				obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough
				evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction
				findings to biomedical relation extraction. 
			</p>

		<div style="text-align:left"> <a class="small" href="https://arxiv.org/abs/2204.04779">
			Online</a></div>
		</li>
		
		<li>
			<span>Ioannis Dikeoulias, Saadullah Amin, and G&uuml;nter Neumann (<strong>2022</strong>)</span>
			<a onclick="toggleItem('ref156Abstract')">Temporal Knowledge Graph Reasoning with Low-rank and Model-agnostic 
				Representations </a>, Proceedings of the 7th Workshop on Representation Learning for NLP. 
				ACL-2022, RepL4NLP May 2022, Pages 111-120 ACL 5/2022 
				(<a href="https://aclanthology.org/2022.repl4nlp-1.12/">RepL4NLP-2022</a>), May, 2022.

		<div class="abstract" id="ref156Abstract" style="display: none;">
			<p>
				Temporal knowledge graph completion (TKGC) has become a popular approach for reasoning over the event and temporal
				knowledge graphs, targeting the completion of knowledge with accurate but missing information. In this context, tensor
				decomposition has successfully modeled interactions between entities and relations. Their effectiveness in static knowledge
				graph completion motivates us to introduce Time-LowFER, a family of parameter-efficient and time-aware extensions of the
				low-rank tensor factorization model LowFER. Noting several limitations in current approaches to represent time, we propose
				a cycle-aware time-encoding scheme for time features, which is model-agnostic and offers a more generalized representation
				of time. We implement our methods in a unified temporal knowledge graph embedding framework, focusing on time-sensitive
				data processing. The experiments show that our proposed methods perform on par or better than the state-of-the-art semantic
				matching models on two benchmarks.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/12361_2022.repl4nlp-1.12.pdf">
			Download</a></div>
		</li>
		
		<li>
			<span>Saadullah Amin, Noon Pokaratsiri, Morgan Wixted, Alejandro Garc&iacute;a-Rudolph, 
				Catalina Mart&iacute;nez-Costa, and G&uuml;nter Neumann (<strong>2022</strong>)</span>
			<a onclick="toggleItem('ref155Abstract')">Few-Shot Cross-lingual Transfer for Coarse-grained De-identification 
				of Code-Mixed Clinical Texts </a>, Proceedings of the 21st Workshop on Biomedical Language Processing. ACL-2022
				BioNLP, May 22-27, Pages 200-211 ACL 5/2022.  
				(<a href="https://aclanthology.org/2022.bionlp-1.20/">BioNLP-2022</a>), May, 2022.

		<div class="abstract" id="ref155Abstract" style="display: none;">
			<p>
				Despite the advances in digital healthcare systems offering curated structured knowledge, much of the critical information
				still lies in large volumes of unlabeled and unstructured clinical texts. These texts, which often contain protected
				health information (PHI), are exposed to information extraction tools for downstream applications, risking patient
				identification. Existing works in de-identification rely on using large-scale annotated corpora in English, which often
				are not suitable in real-world multilingual settings. Pre-trained language models (LM) have shown great potential for
				cross-lingual transfer in low-resource settings. In this work, we empirically show the few-shot cross-lingual transfer
				property of LMs for named entity recognition (NER) and apply it to solve a low-resource and real-world challenge of
				code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke domain. We annotate a gold evaluation
				dataset to assess few-shot setting performance where we only use a few hundred labeled examples for training. Our
				model improves the zero-shot F1-score from 73.7% to 91.2% on the gold evaluation set when adapting Multilingual
				BERT (mBERT) (CITATION) from the MEDDOCAN (CITATION) corpus with our few-shot cross-lingual target corpus. When
				generalized to an out-of-sample test set, the best model achieves a human-evaluation F1-score of 97.2%.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/12360_2022.bionlp-1.20.pdf">
			Download</a></div>
		</li>
		
		<li>
			<span>Stalin Varanasi, Saadullah Amin and G&uuml;nter Neumann (<strong>2021</strong>)</span>
			<a onclick="toggleItem('ref154Abstract')">AutoEQA: Auto-Encoding Questions for Extractive Question Answering</a>, The 2021 Conference on Empirical Methods in Natural Language Processing (<a href="https://2021.emnlp.org/">EMNLP-2021</a>), Nov. 2021.

		<div class="abstract" id="ref154Abstract" style="display: none;">
			<p>
				There has been a significant progress in the
				field of extractive question answering (EQA)
				in the recent years. However, most of them
				rely on annotations of answer-spans in the corresponding
				passages. In this work, we address
				the problem of EQA when no annotations
				are present for the answer span, i.e.,
				when the dataset contains only questions and
				corresponding passages. Our method is based
				on auto-encoding of the question that performs
				a question answering (QA) task during encoding
				and a question generation (QG) task during
				decoding. Our method performs well in a zeroshot
				setting and can provide an additional loss
				that boosts performance for EQA.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/EMNLP_21__AutoEQA.pdf">
			Download</a></div>
		</li>
		
		<li>
			<span>Saadullah Amin and G&uuml;nter Neumann (<strong>2021</strong>)</span>
			<a onclick="toggleItem('ref153Abstract')">T2NER: Transformers based Transfer Learning Framework for Name Entity Recognition</a>, The 16th Conference of the <a href="https://2021.eacl.org/" title="The 16th Conference of the European Chapter of the Association for Computational Linguistics - EACL 2021">European Chapter of the Association for Computational Linguistics</a> (EACL), demo session, 2021.

		<div class="abstract" id="ref153Abstract" style="display: none;">
			<p>
				Recent advances in deep transformer models
				have achieved state-of-the-art in several natural
				language processing (NLP) tasks, whereas
				named entity recognition (NER) has traditionally
				benefited from long-short term memory
				(LSTM) networks. In this work, we present a
				Transformers based Transfer Learning framework
				for Named Entity Recognition (T2NER)
				created in PyTorch for the task of NER with
				deep transformer models. The framework is
				built upon the Transformers library as the core
				modeling engine and supports several transfer
				learning scenarios from sequential transfer
				to domain adaptation, multi-task learning, and
				semi-supervised learning. It aims to bridge the
				gap between the algorithmic advances in these
				areas by combining them with the state-of-theart
				in transformer models to provide a unified
				platform that is readily extensible and can be
				used for both the transfer learning research
				in NER, and for real-world applications. The
				framework is available at: https://github.
				com/suamin/t2ner.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/33_Paper-EACL-2021.pdf">
			Download</a></div>
		</li>
		
		<li>
			<span> Ekaterina Loginova, Stalin Varanasi and G&uuml;nter Neumann (<strong>2021</strong>)</span>
				<a onclick="toggleItem('ref146Abstract')">
					Towards End-to-End Multilingual Question Answering
					</a>.
					In Journal <a href="https://www.springer.com/journal/10796" title="Information Systems Frontiers | Home">Information Systems Frontiers</a>, 23(1): 227-241 (2021).

		<div class="abstract" id="ref146Abstract" style="display: none;">
			<p>
				Multilingual question answering (MLQA) is a critical part of an accessible natural language interface. 
				However, current solutions demonstrate performance far below that of monolingual systems. 
				We believe that deep learning approaches are likely to improve performance in MLQA drastically. 
				This work aims to discuss the current state-of-the-art and remaining challenges. 
				We outline requirements and suggestions for practical parallel data collection and describe existing methods, 
				benchmarks and datasets. We also demonstrate that a simple translation of texts 
				can be inadequate in case of Arabic, English and German languages (on InsuranceQA and SemEval datasets), 
				and thus more sophisticated models are required. We hope 
				that our overview will re-ignite interest in multilingual question answering, especially with regard to neural approaches.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/Towards_Multilingual_Neural_Question_Answering-GN.pdf">
			Download</a> (This is an earlier version of the paper, but content, experiments and results are the same.)</div>
		<a href="https://link.springer.com/article/10.1007/s10796-020-09996-1,">Online version</a>.
	
		</div>
		</li>
		
		<li>
			<span>Stalin Varanasi, Saadullah Amin, and G&uuml;nter Neumann (<strong>2020</strong>)</span>
			<a onclick="toggleItem('ref152Abstract')">CopyBERT: A Unified Approach to Question Generation with Self-Attention</a>
			<a href="https://sites.google.com/view/2ndnlp4convai/program">NLP for Conversational AI - Proceedings of the 2nd Workshop</a>, ACL workshop, 2020.

		<div class="abstract" id="ref152Abstract" style="display: none;">
			<p>
				Contextualized word embeddings provide better
				initialization for neural networks that deal
				with various natural language understanding
				(NLU) tasks including Question Answering
				(QA) and more recently, Question Generation
				(QG). Apart from providing meaningful word
				representations, pre-trained transformer models,
				such as BERT also provide self-attentions
				which encode syntactic information that can
				be probed for dependency parsing and POStagging.
				In this paper, we show that the information
				from self-attentions of BERT are useful
				for language modeling of questions conditioned
				on paragraph and answer phrases.
				To control the attention span, we use semidiagonal
				mask and utilize a shared model for
				encoding and decoding, unlike sequence-tosequence.
				We further employ copy mechanism
				over self-attentions to achieve state-of-the-art
				results for Question Generation on SQuAD
				dataset.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/CopyBert-2020.pdf">
			Download</a></div>
		</li>
		
		<li>
			<span>Saadullah Amin, Stalin Varanasi, Katherine Dunfield and G&uuml;nter Neumann (<strong>2020</strong>)</span>
			<a onclick="toggleItem('ref151Abstract')">LowFER: Low-rank Bilinear Pooling for Link Prediction</a>. 
			<a href="https://icml.cc/Conferences/2020" title="2020 Conference">Proceedings of the 37th International Conference on Machine Learning (ICML-2020), 2020</a>.

		<div class="abstract" id="ref151Abstract" style="display: none;">
			<p>
				Knowledge graphs are incomplete by nature, with only a limited number of observed facts from the world knowledge being represented as structured relations between entities. To partly address this issue, an important task in statistical relational learning is that of link prediction or knowledge graph completion. Both linear and non-linear models have been proposed to solve the problem. Bilinear models, while expressive, are prone to overfitting and lead to quadratic growth of parameters in number of relations. Simpler models have become more standard, with certain constraints on bilinear map as relation parameters. In this work, we propose a factorized bilinear pooling model, commonly used in multi-modal learning, for better fusion of entities and relations, leading to an efficient and constraint-free model. We prove that our model is fully expressive, providing bounds on the embedding dimensionality and factorization rank. Our model naturally generalizes Tucker decomposition based TuckER model, which has been shown to generalize other models, as efficient low-rank approximation without substantially compromising the performance. Due to low-rank approximation, the model complexity can be controlled by the factorization rank, avoiding the possible cubic growth of TuckER. Empirically, we evaluate on real-world datasets, reaching on par or state-of-the-art performance. At extreme low-ranks, model preserves the performance while staying parameter efficient.	
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/11089_lowfer_full_paper_icml2020.pdf">
			Download</a></div>
		</li>
		<li>
			<span> Saadullah Amin, Katherine Dunfield, Anna Vechkaeva and G&uuml;nter Neumann (<strong>2020</strong>)</span>
				<a onclick="toggleItem('ref147Abstract')">
					A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction
					</a>.
					In Proceedings of <a href="https://aclweb.org/aclwiki/BioNLP_Workshop" title="BioNLP Workshop - ACL Wiki">BioNLP-2020</a> at <a href="https://acl2020.org/" title="ACL 2020">ACL-2020</a>.
		<div class="abstract" id="ref147Abstract" style="display: none;">
			<p>
				Fact triples are a common form of structured
				knowledge used within the biomedical domain.
				As the amount of unstructured scientific texts
				continues to grow, manual annotation of these
				texts for the task of relation extraction becomes
				increasingly expensive. Distant supervision
				offers a viable approach to combat this
				by quickly producing large amounts of labeled,
				but considerably noisy, data. We aim to reduce
				such noise by extending an entity-enriched relation
				classification BERT model to the problem
				of multiple instance learning, and defining
				a simple data encoding scheme that significantly
				reduces noise, reaching state-of-the-art
				performance for distantly-supervised biomedical
				relation extraction. Our approach further
				encodes knowledge about the direction of relation
				triples, allowing for increased focus on relation
				learning by reducing noise and alleviating
				the need for joint learning with knowledge
				graph completion.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/BioNLP-23_Paper.pdf">
			Download</a></div>
		<a href="https://github.com/ddemner/BioNLP/blob/master/BioNLP2020.pdf">Online version</a>
	
		</div>
		</li>
		
		
		
		<li>
			<span> Dominik Stammbach and G&uuml;nter Neumann (<strong>2019</strong>)</span>
				<a onclick="toggleItem('ref145Abstract')">
					Team DOMLIN: Exploiting Evidence Enhancement for the FEVER Shared Task.
					</a>
					In Proceedings of the <a href="https://www.aclweb.org/anthology/volumes/D19-66/" 
					title="Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER) - ACL Anthology">
					Second Workshop on Fact Extraction and VERification</a> (FEVER), EMNLP workshop, 2019.

		<div class="abstract" id="ref145Abstract" style="display: none;">
			<p>
				This paper contains our system description for the second Fact Extraction and VERification (FEVER) challenge. 
				We propose a two-staged sentence selection strategy to account for examples in the dataset where evidence is not only 
				conditioned on the claim, but also on previously retrieved evidence. We use a publicly available 
				document retrieval module and have fine-tuned BERT checkpoints for sentence 
				selection and as the entailment classifier. We report a FEVER score of 68.46% on the blind testset.
			</p>
	
		<div style="text-align:left"> <a class="small" href="publications/new-ps/D19-6616.pdf">
			Download</a></div>
		<a href="https://www.aclweb.org/anthology/D19-6616/">Online version</a>.
	
		</div>
		</li>
		
		<li>
		<span>Saadullah Amin, G&uuml;nter Neumann, Katherine Dunfield, Anna Vechkaeva, 
			Kathryn Annette Chapman, and Morgan Kelly Wixted (<strong>2019</strong>)</span>
			<a onclick="toggleItem('ref143Abstract')">
				MLT-DFKI at CLEF eHealth 2019: Multi-label Classification of ICD-10 Codes with BERT</a>. 
			In working notes of <a href="https://clefehealth.imag.fr/" title="CLEF eHealth 2019">CLEF eHealth</a>, 2019.

	<div class="abstract" id="ref143Abstract" style="display: none;">
		<p>
			With the adoption of electronic health record (EHR) systems,
			hospitals and clinical institutes have access to large amounts
			of heterogeneous patient data. Such data consists of structured
			(insurance details, billing data, lab results etc.) and unstructured
			(doctor notes, admission and discharge details, medication steps etc.)
			documents, of which, latter is of great significance to apply natural
			language processing (NLP) techniques. In parallel, recent advancements
			in transfer learning for NLP has pushed the state-of-the-art to new
			limits on many language understanding tasks. Therefore, in this paper,
			we present team DFKI-MLT's participation at CLEF eHealth 2019 Task 1 of
			automatically assigning ICD-10 codes to non-technical summaries (NTSs)
			of animal experiments where we use various architectures in multi-label
			classification setting and demonstrate the effectiveness of transfer
			learning with pre-trained language representation model BERT
			(Bidirectional Encoder Representations from Transformers) and its recent
			variant BioBERT. We first translate task documents from German to
			English using automatic translation system and then use BioBERT which
			achieves an F1-micro of 73.02% on submitted run as
			evaluated by the challenge.
		</p>
	
	<div style="text-align:left"> <a class="small" href="publications/new-ps/paper_67.pdf">
		Download</a></div>
	<a href="http://ceur-ws.org/Vol-2380/paper_67.pdf">Online version</a>.
	
	</div>
	</li>
	
	
		<li>
			<span>Alejandro Figueroa, Carlos G&oacute;mez-Pantoja, and G&uuml;nter Neumann (<strong>2019</strong>)</span>
			<a onclick="toggleItem('ref139Abstract')">Integrating heterogeneous sources for predicting question temporal anchors across Yahoo! Answers</a>. 
			In journal <a href="https://www.sciencedirect.com/journal/information-fusion/vol/50/suppl/C" title="ScienceDirect">Information Fusion, Volume 50</a>, 
			October 2019, Pages 112-125.

		<div class="abstract" id="ref139Abstract" style="display: none;">
			<p>
				Modern Community Question Answering (CQA) web forums provide the possibility to browse their archives using
				question-like search queries as in Information Retrieval
				(IR) systems. Although these traditional IR methods have become very successful at fetching semantically related
				questions, they typically leave unconsidered their
				temporal relations. That is to say, a group of questions may be asked more often during specific recurring time lines
				despite being semantically unrelated. In fact,
				predicting temporal aspects would not only assist these platforms in widening the semantic diversity of their search
				results, but also in re-stating questions that
				need to refresh their answers and in producing more dynamic, especially temporally-anchored, displays.<br/><br/>

				In this paper, we devised a new set of time-frame specific categories for CQA questions, which is obtained by fusing
				two distinct earlier taxonomies (i.e., [29] and
				[50]). These new categories are then utilized in a large crowd-sourcing based human annotation effort. Accordingly, we
				present a systematical analysis of its results
				in terms of complexity and degree of difficulty as it relates to the different question topics.<br/><br/>

				Incidentally, through a large number of experiments, we investigate the effectiveness of a wider variety of linguistic
				features compared to what has been done in
				previous works. We additionally mix evidence/features distilled directly and indirectly from questions by capitalizing
				on their related web search results. We
				finally investigate the impact and effectiveness of multi-view learning to boost a large variety of multi-class
				supervised learners by optimizing a latent layer
				build on top of two views: one composed of features harvested from questions, and the other from CQA meta data and
				evidence extracted from web resources (i.e., 
				nippets and Internet archives).
			</p>
		
		<div style="text-align:left"> <a class="small" href="publications/new-ps/Temporality-accepted.pdf">Download accepted
			manuscript</a></div>
		<div style="text-align:left"><a href="https://www.sciencedirect.com/science/article/pii/S1566253518303415?via%3Dihub">Online
			version: https://doi.org/10.1016/j.inffus.2018.10.006</a></div>
		
		</div>
		</li>
		
		<li>
			<span>Ekaterina Loginova and G&uuml;nter Neumann (<strong>2018</strong>)</span>
			<a onclick="toggleItem('ref138Abstract')">An Interactive Web-Interface for Visualizing the Inner Workings of the Question Answering LSTM</a>. 
			In proceedings of the <a href="http://emnlp2018.org/" title="2018 Conference on Empirical Methods in Natural Language Processing - EMNLP 2018">
				Conference on Empirical Methods in Natural Language Processing - EMNLP-2018</a>, October 31 – November 4, Brussels, Belgium, 2018.

		<div class="abstract" id="ref138Abstract" style="display: none;">
			<p>
				Deep learning models for NLP are potent but not readily interpretable. It prevents researchers from improving a model’s performance
				efficiently and users from applying it for a task which requires a high level of trust in the system. We present a visualisation tool
				which aims to illuminate the inner workings of a specific LSTM model for question answering.
				It plots heatmaps of neurons’ firings and allows a user to check the dependency between neurons and manual features. The system possesses
				an interactive web-interface and can be adapted to other models and domains.
			</p>
		
		<div style="text-align:right"> <a class="small" href="publications/new-ps/emnlp-demo.pdf">Download paper</a></div>
		</div>
		</li>
		
		<li>
			<span>Georg Heigold, Stalin Varanasi, G&uuml;nter Neumann and Josef van Genabith (<strong>2018</strong>)</span>
			<a onclick="toggleItem('ref135Abstract')">How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?</a>. 
			In proceedings of <a href="http://www.conference.amtaweb.org/" title="AMTA 2018">AMTA</a>, March.

		<div class="abstract" id="ref135Abstract" style="display: none;">
			<p>
				This paper investigates the robustness of NLP against perturbed word forms. While neural approaches can achieve (almost) 
				human-like accuracy for certain tasks and conditions, they often are sensitive to small changes in the input such as non-canonical input 
				(e.g., typos). Yet both stability and robustness are desired properties in applications involving user-generated content, 
				and the more as humans easily cope with such noisy or adversary conditions. In this paper, we study the impact of noisy input. 
				We consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions 
				for training and testing. Moreover, we empirically evaluate the robustness of different models (convolutional neural networks, 
				recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), 
				and different NLP tasks (morphological tagging, machine translation). 
			</p>
		
		<a href="https://arxiv.org/pdf/1704.04441.pdf" title="">Online version</a>
		</div>
		</li>
		
		
		<li>
			<span>Georg Heigold, G&uuml;nter Neumann and Josef van Genabith (<strong>2017</strong>)<a name="EACL-2017"></a></span>
			<a onclick="toggleItem('ref132Abstract')">An Extensive Empirical Evaluation of Character-Based Morphological Tagging for 14 Languages</a>. 
			In proceedings of <a href="http://eacl2017.org/" title="EACL2017 - Home">EACL</a>, 2017.
	
		<div class="abstract" id="ref132Abstract" style="display: none;">
			<p>
				This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets. 
				Character-based approaches are attractive as they can handle rarely- and unseen words gracefully. We evaluate on 14 
				languages and observe consistent gains over a state-of-the-art morphological tagger across all languages except 
				for English and French, where we match the state-of-the-art. We compare two architectures for computing 
				character-based word vectors using recurrent (RNN) and convolutional (CNN) nets. We show that the CNN based approach 
				performs slightly worse and less consistently than the RNN based approach. 
				Small but systematic gains are observed when combining the two architectures by ensembling.
			</p>
			
		<a href="http://www.aclweb.org/anthology/E/E17/E17-1048.pdf" title="">Online version</a>
		<a class="small" href="publications/new-ps/eacl-2017.pdf">Download preprint</a>
		</div>
	
</ul>
	
</div></div></div>

<div id="footer" class="clear">
	<p class="left">&copy; 
		<script type="text/javascript">
			var today = new Date()
			var year = today.getFullYear()
			document.write(year)
		</script>
	G&uuml;nter Neumann</p>
	<p class="right">
		<a href="http://validator.w3.org/check?uri=referer"><img
			src="http://www.w3.org/Icons/valid-xhtml10-blue"
			alt="Valid XHTML 1.0 Strict" height="31" width="88" /></a>
		<a href="https://www.dfki.de/web/data-protection-en">Data Protection</a>
		<a href="https://www.dfki.de/web/legal-info-en">Legal Information</a>
	</p>
	</div>
</div>
						
</body>
</html>
